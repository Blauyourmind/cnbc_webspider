{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import scrapy\n",
    "import pandas as pd\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "# ***** NOTE: you MUST restart the kernal everytime you want to run the spider *****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Web Crawler Class\n",
    "class CNBCSpider(scrapy.Spider):\n",
    "    name = 'CNBC_spider'\n",
    "    \n",
    "    def start_requests(self):\n",
    "        \n",
    "        url = 'https://www.cnbc.com/'\n",
    "        yield scrapy.Request(url= url, callback= self.parse_front_page) \n",
    "    \n",
    "    def parse_front_page(self, response):\n",
    "        #open_in_browser(response)\n",
    "        headlines = [x for x in response.css('a::attr(href)').extract() if 'cnbc.com/2020/' in x or 'cnbc.com/2021/' in x]\n",
    "\n",
    "        for link in headlines:\n",
    "            yield response.follow(url=link, callback= self.parse_article)\n",
    "    \n",
    "    def parse_article(self, response):\n",
    "        \n",
    "        # get article headline\n",
    "        headline = response.css('h1.ArticleHeader-headline::text').extract()\n",
    "        \n",
    "        # get article dates\n",
    "        date = response.css('time[data-testid=\"published-timestamp\"]::text').extract()\n",
    "        \n",
    "        if len(headline) != 0 and len(date) != 0 :\n",
    "            headlines.append(headline)\n",
    "            dates.append(date)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate arrays to store data from the web crawler \n",
    "headlines, descriptions, dates = [], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running the Spider \n",
    "\n",
    "# initiate a crawler process\n",
    "process = CrawlerProcess()\n",
    "    \n",
    "# tell the process which spider to use\n",
    "process.crawl(CNBCSpider)\n",
    "\n",
    "# start the crawling process\n",
    "process.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse Dates and Headlines\n",
    "parsed_dates = [x[0].split(',')[1].strip() for x in dates]\n",
    "parsed_headlines = [x[0] for x in headlines]\n",
    "\n",
    "# build a df out of new scraped data \n",
    "df = pd.DataFrame(data={'publish_date':parsed_dates,\n",
    "                        'headline':parsed_headlines})\n",
    "# convert date column in datetime objects\n",
    "df['publish_date'] = pd.to_datetime(df['publish_date'],infer_datetime_format=True)\n",
    "\n",
    "# set the date column as the df index\n",
    "df.set_index('publish_date',inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read old data df\n",
    "df_old = pd.read_csv('cnbc_news.csv',parse_dates=['publish_date'], index_col='publish_date')\n",
    "\n",
    "# concat new df to old df\n",
    "combined_df = pd.concat([df_old,df], axis=0)\n",
    "\n",
    "# remove duplicate headlines if they exist\n",
    "combined_df.drop_duplicates(subset =\"headline\", keep = 'last', inplace = True) \n",
    "\n",
    "# sort df by date\n",
    "combined_df.sort_values(by='publish_date', inplace=True)\n",
    "\n",
    "# export file back to .CSV\n",
    "combined_df.to_csv('cnbc_news.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in and check new combined dataset from wsi_news.csv \n",
    "full_df = pd.read_csv('cnbc_news.csv',parse_dates=['publish_date'], index_col='publish_date')\n",
    "full_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
