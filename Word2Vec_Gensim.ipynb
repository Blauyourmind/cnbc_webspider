{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = [word for word in set(stopwords.words('english'))] # get stopwords from nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(s, lower=True, strip_punc=True):\n",
    "    '''\n",
    "    Input: String, lower(Bool), strip_punc(Bool)\n",
    "    Output: List of Strings\n",
    "    '''\n",
    "    punc = '.-,?<>:;\"\\'!%'\n",
    "    if isinstance(s, str):\n",
    "        s = s.split() # tokenize text\n",
    "    if lower:\n",
    "        s = [t.lower() for t in s]\n",
    "    if strip_punc:\n",
    "        s = [t.strip(punc) for t in s]\n",
    "        \n",
    "    return s\n",
    "\n",
    "def token_frequency(tokens, tf= None, relative=False):\n",
    "    \"\"\"\n",
    "    Inputs: \n",
    "        tokens = List of Strings or None\n",
    "        tf = dict or None\n",
    "        relative = Boolean\n",
    "    Output: \n",
    "        Dictionary of a token frequencies\n",
    "    \"\"\"\n",
    "    tf = {} if tf==None else tf\n",
    "    \n",
    "    if len(tf) != 0 and relative==True:\n",
    "        if isinstance(list(tf.items())[0][1], float):\n",
    "            print('WARNING: Adding raw counts to relative frequency')\n",
    "            return tf\n",
    "        \n",
    "    for token in tokens:\n",
    "        if token in tf:\n",
    "            tf[token] += 1\n",
    "        else:\n",
    "            tf[token] = 1\n",
    "    \n",
    "    if relative:\n",
    "        total = sum([v for k,v in tf.items()])\n",
    "        tf = {k:v/total for k, v in tf.items()}\n",
    "          \n",
    "    return tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in and Tokenize New Headlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>publish_date</th>\n",
       "      <th>headline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16910</th>\n",
       "      <td>2020-07-02</td>\n",
       "      <td>[morgan, stanley, analyst, upgrades, avis, bud...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16911</th>\n",
       "      <td>2020-07-02</td>\n",
       "      <td>[tesla, stock, hits, record, high, and, smashe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16912</th>\n",
       "      <td>2020-07-02</td>\n",
       "      <td>[fiat, chrysler, unveils, dodge, durango, hell...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16913</th>\n",
       "      <td>2020-07-02</td>\n",
       "      <td>[bar, owners, reckon, with, costly, stop, and,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16914</th>\n",
       "      <td>2020-07-02</td>\n",
       "      <td>[texas, issues, statewide, order, requiring, f...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      publish_date                                           headline\n",
       "16910   2020-07-02  [morgan, stanley, analyst, upgrades, avis, bud...\n",
       "16911   2020-07-02  [tesla, stock, hits, record, high, and, smashe...\n",
       "16912   2020-07-02  [fiat, chrysler, unveils, dodge, durango, hell...\n",
       "16913   2020-07-02  [bar, owners, reckon, with, costly, stop, and,...\n",
       "16914   2020-07-02  [texas, issues, statewide, order, requiring, f..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in cnbc articles\n",
    "df = pd.read_csv('data/cnbc_news.csv',parse_dates=['publish_date'])\n",
    "df['publish_date'] = df['publish_date'].dt.tz_convert(None)\n",
    "# drop any rows with null\n",
    "df = df.dropna()\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "# preprocess text in headlines\n",
    "df['headline'] = df['headline'].apply(lambda x: preprocess(x))\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec With Gensim\n",
    "\n",
    "- https://radimrehurek.com/gensim/models/word2vec.html\n",
    "- https://towardsdatascience.com/a-beginners-guide-to-word-embedding-with-gensim-word2vec-model-5970fa56cc92"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list(['how', 'to', 'play', 'a', 'stock', 'market', 'that', 'may', 'be', 'stuck', 'in', 'place', 'for', 'a', 'while']),\n",
       "       list(['stock', 'market', 'live', 'updates', 'stocks', 'give', 'up', 'gains', 'apple', 'to', 'reclose', 'some', 'stores', 'spotify', 'pops'])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gensim takes a list of lists input: a list of tokenized documents\n",
    "# here, we will treat news headlines as documents\n",
    "docs = df['headline'].values\n",
    "docs[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a word2vec model with gensim\n",
    "# size: number of dimension of the embeddings: default is 100 (dimensions as in tokens per vector)\n",
    "# window: the max distance between a target word and words around the target, default is 5\n",
    "# min_count: min count of words to consider when traning the model; words with occurance less than this count will be ignored.\n",
    "        # default is 5\n",
    "# workers: number of partitions during training and default is 3\n",
    "# sg: the traingin algo to use: Either CBOW(0) or skipgram (1)\n",
    "\n",
    "model = Word2Vec(docs, size=50, window=3, min_count=5, workers=3, sg=1)\n",
    "vecs = model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.46253225, -0.17376314, -0.5132673 , -0.08568393, -0.2092115 ,\n",
       "        0.06243603,  0.16513984,  0.16365807, -0.2592522 ,  0.04411814,\n",
       "       -0.05382431, -0.03621269, -0.33821273, -0.00457379,  0.05200712,\n",
       "       -0.1989373 , -0.23323207, -0.06765874,  0.04428462,  0.06372458,\n",
       "        0.04689382,  0.4443095 ,  0.01307581, -0.59132373,  0.16506147,\n",
       "        0.18172164, -0.14536189,  0.14935048, -0.35808894,  0.12052608,\n",
       "       -0.04363895,  0.10720734, -0.01859086, -0.09799474,  0.27447236,\n",
       "        0.08243097, -0.07105912, -0.06738936, -0.11797523, -0.35667685,\n",
       "        0.18708815,  0.02740279, -0.0369225 , -0.23976038,  0.28822368,\n",
       "       -0.02022022, -0.54944307, -0.45074967,  0.26717478,  0.10301977],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vecs['bull']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9426507"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cosine similarity: 1 very similar and 0 not similar at all\n",
    "vecs.similarity('bull','crash')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('covid-19', 0.8384757041931152),\n",
       " ('virus', 0.8301467895507812),\n",
       " ('global', 0.7958473563194275),\n",
       " ('us', 0.7786386013031006),\n",
       " ('slow', 0.719185471534729),\n",
       " ('early', 0.7063236236572266),\n",
       " ('trial', 0.7009888291358948),\n",
       " ('lockdowns', 0.6995306611061096),\n",
       " ('canceled', 0.6964773535728455),\n",
       " ('antibody', 0.6947957277297974)]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# most similar word\n",
    "vecs.most_similar('coronavirus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save a moel\n",
    "# model.save(word2vec.model)\n",
    "\n",
    "# load model\n",
    "# model = Word2Vec.load(\"word2vec.model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
